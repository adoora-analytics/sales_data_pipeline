# ğŸ“Š Sales Data Pipeline â€“ ETL with Python & PostgreSQL

This repository contains my **first data engineering project** : a simple but complete ETL (Extractâ€“Transformâ€“Load) pipeline for processing sales data.  
The pipeline extracts raw CSV files, cleans and transforms them using Python, validates the data, and loads the results into a PostgreSQL **staging table** and an optional **data warehouse** built using SQL scripts.

This project represents my starting point in data engineering, and I will continue improving it as I learn.


---

## ğŸš€ What This Project Shows

**Skills demonstrated:**

- Python-based ETL orchestration (`main.py`)
- Modular pipeline design (`etl/` and `utils/` packages)
- Reading and combining multiple CSV files
- Data cleaning and type conversion with pandas
- Basic data validation
- Loading into PostgreSQL using a reusable DB helper
- Config-driven design (`config.json` + `.env`)
- Logging to file for debugging and traceability

---

## ğŸ§± Project Structure

```text
sales_data_pipeline/
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.json             # Local config (ignored) 
â”‚   â””â”€â”€ config.example.json     # Template config for others
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Input CSVs
â”‚
â”œâ”€â”€ dwh/
â”‚   â”œâ”€â”€ dwh.sql                 # DWH star-schema (fact + dimensions) creation (includes indexing too)
â”‚   â”œâ”€  dwh_population.sql      # Scripts to populate sales dimension and fact tables 
â”‚   â””â”€â”€ dwh_optimization.sql    # Basic performance tuning
â”‚
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extract.py              # Reads CSVs into DataFrames
â”‚   â”œâ”€â”€ transform.py            # Cleans, normalises, enriches data
â”‚   â”œâ”€â”€ validate.py             # Data quality checks
â”‚   â””â”€â”€ load.py                 # Loads into PostgreSQL staging table
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ pipeline.log            # Log output (ignored in git)
â”‚
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py               # Reads config + env variables
â”‚   â”œâ”€â”€ db.py                   # DB connection & helpers
â”‚   â””â”€â”€ logger.py               # Logging configuration
â”‚
â”œâ”€â”€ .env                        # Local secrets (ignored)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ main.py                     # Orchestrates the ETL steps
â””â”€â”€ staging.sql                 # DDL for the staging table
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ“˜ How to Use This Project

Follow these steps to run the ETL pipeline and reproduce the results on your own machine.

---

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/YOUR_USERNAME/sales_data_pipeline.git
cd sales_data_pipeline
```

---

2ï¸âƒ£ Create and Activate a Virtual Environment (Recommended)

python -m venv .venv
# Windows
.\.venv\Scripts\activate
# macOS/Linux
source .venv/bin/activate


---

3ï¸âƒ£ Install Required Dependencies

pip install -r requirements.txt


---

4ï¸âƒ£ Set Up Database Credentials

Create a local .env file in the project root:

DB_PASSWORD=your_postgres_password



---

5ï¸âƒ£ Configure the Pipeline

Copy the example config file:

cp config/config.example.json

Then edit:

{
    "db_name": "your_database",
    "db_user": "postgres",
    "db_host": "localhost",
    "db_port": 5432,
    "raw_data_path": "data/raw"
}


---

6ï¸âƒ£ Add Raw CSV Files

Place your datasets inside:

data/raw/
    sales_2024.csv
    sales_2025.csv

Column names should match those expected in transform.py.


---

7ï¸âƒ£ Create the Staging Table

Run this SQL in PostgreSQL:

psql -U postgres -d your_database -f sql/day7/staging.sql


---

8ï¸âƒ£ Run the ETL Pipeline

python main.py

This will:

1. Extract raw CSVs


2. Clean and transform the data


3. Validate column types + nulls + duplicates


4. Load everything into w4d7_staging


5. Save logs to logs/pipeline.log




---

ğŸ› Data Warehouse Layer (Optional)

Inside the dwh/ folder:

dwh.sql â€“ creates fact + dimension tables while indexing 

dwh_population.sql â€“ populates dimensions and fact tables

dwh_optimization.sql â€“ performance tuning


Run them in order:

psql -U postgres -d your_database -f dwh/dwh.sql
psql -U postgres -d your_database -f dwh/dwh_population.sql
psql -U postgres -d your_database -f dwh/dwh_optimization.sql


---

ğŸ§­ Pipeline Architecture (Simple Diagram)

CSV Files
   â”‚
   â–¼
extract.py â†’ transform.py â†’ validate.py â†’ load.py
                                          â”‚
                                          â–¼
                               PostgreSQL (staging)
                                          â”‚
                                          â–¼
                                       DWH Scripts
                                (fact + dimension tables)


